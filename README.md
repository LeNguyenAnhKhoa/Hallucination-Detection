# A.M.P at SciHal2025: Automated Hallucination Detection in Scientific Content via LLMs and Prompt Engineering

This repository provides the code for the A.M.P system for the SDP share task.

## Task: SciHal2025

- SciHal stands for "Hallucination Detection for Scientific Content". The task is to detect hallucinated claims in the answers to scientific questions generated by GenAI-powered research assistants. Given the claims and the references refer to them, the system need to detect whether the references Entail/Contradict/Unverifiable to the claims. To see more relationships between claims and references, see the image below for more details.
- This code is for our hallucination detection pipeline that utilizes open large language models for the shared task hosted by the 5-th SDP workshop (co-located with ACL). For more details about the task and dataset, please refer to [the shared task page](https://sdproc.org/2025/scihal.html).
<p align="center"><img src="https://github.com/user-attachments/assets/f8a7d554-de7b-4098-b3d3-da036e3acf26"></p>

## Method: A.M.P

- Our system focuses on prompting for LLMs like o3-mini or gemini-2.5-flash. We use few-shot prompting technique and define fields for LLMs like task definition or output concepts.
- These image below show the example prompt for 3 classes: Entailment, Contradiction and Unverifiable. To see all the prompts for each subtask refer to the `.txt` files in the `subtask1` and `subtask2` folders.

<p align="center"><img src="https://github.com/user-attachments/assets/7e709bcf-f437-436f-9b11-f15976bdd8d6"></p>

- Instead of directly for subtask 1, we based on the decision tree above took the prediction result directly from subtask 2 and reduced it to subtask 1. This increases the achieved result and makes the prediction simpler.
<p align="center"><img src="https://github.com/user-attachments/assets/b6dacc89-2cba-47fe-8806-b6665238f8af"></p>

## Data and Model
- For the raw training data, see the `.csv` file in folder `Data` or visit on [Kaggle page](https://www.kaggle.com/competitions/hallucination-detection-scientific-content-2025/data) for the full dataset.
- We use models through API. With all `.py` file, set your **API_KEY** before running. For Gemini models, see documents at [Google AI](https://ai.google.dev/gemini-api/docs?authuser=3) to better understand how to use the API. For OpenAI models, see documents at [OpanAI platform](https://platform.openai.com/docs/overview).
## How to Run
### Installation
```bash
git clone https://github.com/LeNguyenAnhKhoa/Hallucination-Detection.git
cd Hallucination-Detection
pip install -r requirements.txt
```
### Data preprocessing
```python3
python clean_data.py --subtask1 './Data/subtask1_train_batch3.csv' --subtask2 './Data/subtask2_train_batch3.csv' --output './Data/cleaned_data3.csv'
```
### Prediction
- For Gemini models:
```python3
python gemini_inference.py --model 'gemini-2.5-flash-preview-04-17' --output './Result/gem2_3.csv' --sleep_time 6.1
```
- For OpenAI models:
```python3
python gpt_inference.py --model 'gpt-4o-mini' --output './Result/gpt4o.csv' --sleep_time 0.3
```
### Evaluation
```python3
python score.py --file './Result/gpt-4o-mini.csv'
```
### Submission
- View files in folder `notebook` to easily run prediction and evaluation.
- We also put the testing and evaluation code of the models in these files for easier running.
